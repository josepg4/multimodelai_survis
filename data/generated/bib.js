define({ entries : {
    "Beck2016Visual": {
        "abstract": "Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.",
        "author": "Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel",
        "doi": "10.1109/TVCG.2015.2467757",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}",
        "type": "article",
        "url": "http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/vast15-survis.pdf",
        "volume": "22",
        "year": "2016"
    },
    "alayrac_flamingo_2022": {
        "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen",
        "doi": "10.48550/arXiv.2204.14198",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\SZXZRQBK\\\\Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Lea.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\PG3F4ICS\\\\2204.html:text/html",
        "keywords": "Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning",
        "month": "nov,",
        "note": "arXiv:2204.14198 [cs]",
        "publisher": "arXiv",
        "shorttitle": "Flamingo",
        "title": "Flamingo: a {Visual} {Language} {Model} for {Few}-{Shot} {Learning}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2204.14198",
        "urldate": "2024-05-16",
        "year": "2022"
    },
    "brown_language_2020": {
        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
        "author": "Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",
        "doi": "10.48550/arXiv.2005.14165",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\6RS6DBU3\\\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\K5NUZ3XX\\\\2005.html:text/html",
        "keywords": "Computer Science - Computation and Language",
        "month": "jul,",
        "note": "arXiv:2005.14165 [cs]",
        "publisher": "arXiv",
        "title": "Language {Models} are {Few}-{Shot} {Learners}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2005.14165",
        "urldate": "2024-05-15",
        "year": "2020"
    },
    "li_blip-2_2023": {
        "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "doi": "10.48550/arXiv.2301.12597",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\J2YLHXG6\\\\Li et al. - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training .pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\WXYQWZ9N\\\\2301.html:text/html",
        "keywords": "Computer Science - Computer Vision and Pattern Recognition",
        "month": "jun,",
        "note": "arXiv:2301.12597 [cs]",
        "publisher": "arXiv",
        "shorttitle": "{BLIP}-2",
        "title": "{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2301.12597",
        "urldate": "2024-05-16",
        "year": "2023"
    },
    "li_llava-med_2023": {
        "abstract": "Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.",
        "author": "Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng",
        "doi": "10.48550/arXiv.2306.00890",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\KM9FYKKF\\\\Li et al. - 2023 - LLaVA-Med Training a Large Language-and-Vision As.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\ZTBK4AQX\\\\2306.html:text/html",
        "keywords": "Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition",
        "month": "jun,",
        "note": "arXiv:2306.00890 [cs]",
        "publisher": "arXiv",
        "shorttitle": "{LLaVA}-{Med}",
        "title": "{LLaVA}-{Med}: {Training} a {Large} {Language}-and-{Vision} {Assistant} for {Biomedicine} in {One} {Day}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2306.00890",
        "urldate": "2024-05-16",
        "year": "2023"
    },
    "liu_improved_2023": {
        "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in {\\textasciitilde}1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "doi": "10.48550/arXiv.2310.03744",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\XW9DYB4P\\\\Liu et al. - 2023 - Improved Baselines with Visual Instruction Tuning.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\TUXVQINY\\\\2310.html:text/html",
        "keywords": "Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning",
        "month": "oct,",
        "note": "arXiv:2310.03744 [cs]",
        "publisher": "arXiv",
        "title": "Improved {Baselines} with {Visual} {Instruction} {Tuning}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2310.03744",
        "urldate": "2024-05-16",
        "year": "2023"
    },
    "liu_visual_2023": {
        "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "doi": "10.48550/arXiv.2304.08485",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\CM7UE5NE\\\\Liu et al. - 2023 - Visual Instruction Tuning.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\LCQFVGA5\\\\2304.html:text/html",
        "keywords": "Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, type:inproceedings",
        "month": "dec,",
        "note": "arXiv:2304.08485 [cs]",
        "publisher": "arXiv",
        "title": "Visual {Instruction} {Tuning}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2304.08485",
        "urldate": "2024-05-15",
        "year": "2023"
    },
    "radford_learning_2021": {
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya",
        "doi": "10.48550/arXiv.2103.00020",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\2SZCD62G\\\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\RBZ3CZY2\\\\2103.html:text/html",
        "keywords": "Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning",
        "month": "feb,",
        "note": "arXiv:2103.00020 [cs]",
        "publisher": "arXiv",
        "title": "Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2103.00020",
        "urldate": "2024-05-15",
        "year": "2021"
    },
    "touvron_llama_2023": {
        "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth\u00e9e and Rozi\u00e8re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume",
        "doi": "10.48550/arXiv.2302.13971",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\GN863QBF\\\\Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\UBXJTB92\\\\2302.html:text/html",
        "keywords": "Computer Science - Computation and Language",
        "month": "feb,",
        "note": "arXiv:2302.13971 [cs]",
        "publisher": "arXiv",
        "shorttitle": "{LLaMA}",
        "title": "{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2302.13971",
        "urldate": "2024-05-16",
        "year": "2023"
    },
    "zhang_video-llama_2023": {
        "abstract": "We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "doi": "10.48550/arXiv.2306.02858",
        "file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\P9JDN4XC\\\\Zhang et al. - 2023 - Video-LLaMA An Instruction-tuned Audio-Visual Lan.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\id4ge\\\\Zotero\\\\storage\\\\LBY53IE7\\\\2306.html:text/html",
        "keywords": "Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",
        "month": "oct,",
        "note": "arXiv:2306.02858 [cs, eess]",
        "publisher": "arXiv",
        "shorttitle": "Video-{LLaMA}",
        "title": "Video-{LLaMA}: {An} {Instruction}-tuned {Audio}-{Visual} {Language} {Model} for {Video} {Understanding}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2306.02858",
        "urldate": "2024-05-16",
        "year": "2023"
    }
}});